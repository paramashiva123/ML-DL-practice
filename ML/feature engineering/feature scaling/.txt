

 1. What is **Feature Scaling**?

**Feature Scaling** is the process of *normalizing* or *standardizing* the range of independent variables (features) so that they fit within a particular scale â€” often [0, 1] or [-1, 1].

Why?  
Because many machine learning algorithms (especially distance-based ones like KNN, SVM, or gradient-based optimizers) perform **better and faster** when features are on a similar scale.


# 2. Common **Feature Scaling Techniques**

| Technique | Description | Formula |
|:---|:---|:---|
| **Min-Max Scaling** | Rescales the feature to a fixed range, usually [0, 1] | \( X' = \frac{X - X_{min}}{X_{max} - X_{min}} \) |
| **Standardization (Z-score normalization)** | Centers data around mean = 0 and standard deviation = 1 | \( X' = \frac{X - \mu}{\sigma} \) |
| **MaxAbs Scaling** | Scales data to [-1, 1] by dividing by the maximum absolute value | \( X' = \frac{X}{\text{max}(\lvert X \rvert)} \) |
| **Robust Scaling** | Uses median and IQR, robust to outliers | \( X' = \frac{X - \text{median}(X)}{\text{IQR}(X)} \) |


# 3. **Feature Transformation**

Feature Transformation is **changing the form or distribution** of the features to make them more suitable for modeling.  
This is often used when the data is skewed, nonlinear, or does not meet assumptions of the model.


# 4. Common **Feature Transformation Techniques**

| Technique | Use Case | Example Transformation |
|:---|:---|:---|
| **Log Transform** | Reduces right skewness | \( X' = \log(X+1) \) (we add 1 to avoid log(0)) |
| **Square Root Transform** | Reduces moderate skewness | \( X' = \sqrt{X} \) |
| **Box-Cox Transform** | Makes data more normal | Specialized formula depending on parameter \( \lambda \) |
| **Yeo-Johnson Transform** | Like Box-Cox, but supports zero/negative values | Specialized formula |
| **Power Transform** | Stabilizes variance, normalizes | Various methods like Box-Cox and Yeo-Johnson |


# 5. **When to Use Scaling vs Transformation?**

| Situation | Scaling | Transformation |
|:---|:---|:---|
| Features have vastly different ranges | Yes | Maybe |
| Features are skewed / non-normal | No | Yes |
| Model is distance-based (KNN, SVM) | Yes | No |
| Model is tree-based (Random Forest, XGBoost) | No | No (but maybe transformation helps) |
| Model needs normally distributed data (Linear Regression, Logistic Regression) | Maybe | Yes |


# 6. **Real-World Example**

Suppose you have two features:

- `age` (18-100 years)
- `salary` (20,000 - 200,000)

If you use these **directly** in a KNN model:

- Distance calculations will be dominated by **salary**.
- So, you apply **Min-Max Scaling** to both.

Suppose your **salary** is very **right-skewed** (many people earn low, few people earn a lot):

- You apply a **log transformation** to make the distribution more normal.

**Often**, both scaling **and** transformation are used together!


# 7. Quick Summary Table

| Step | Action |
|:---|:---|
| Check feature ranges | Scale if needed |
| Check feature distributions | Transform if skewed |
| Check model requirements | Scale for distance-based models, transform for normality-based models |
